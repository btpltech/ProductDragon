#!/bin/sh

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


# Create a Hadoop AMI. Runs on the EC2 instance.

# Import variables
bin=`dirname "$0"`
bin=`cd "$bin"; pwd`
. "$bin"/hadoop-ec2-env.sh

# Remove environment script since it contains sensitive information
#sudo rm -f "$bin"/hadoop-ec2-env.sh

# Install Java
echo "Downloading and installing java binary."
#ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo chmod -R 777 /usr/local
cd /usr/local
sudo wget -nv  $JAVA_BINARY_URL
sudo tar xzf jdk-7u17-linux-x64.tar.gz
#sh java.bin
sudo rm -f jdk-7u17-linux-x64.tar.gz

# Install tools
sudo apt-get update
echo "Installing Ant"
sudo apt-get install ant
echo "Installing rpms."
echo "downloading yum"
sudo apt-get install yum
echo "installing yum"
sudo yum -y install rsync lynx screen ganglia-gmetad ganglia-gmond ganglia-web httpd php
sudo yum -y clean all

# Install Hadoop
echo "Installing Hadoop $HADOOP_VERSION."
#ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo chmod -R 777 /usr/local
cd /usr/local
sudo wget -nv http://archive.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz
[ ! -f hadoop-$HADOOP_VERSION.tar.gz ] && wget -nv http://www.apache.org/dist/hadoop/core/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz
sudo tar -zxvf hadoop-$HADOOP_VERSION.tar.gz
sudo rm -f hadoop-$HADOOP_VERSION.tar.gz
sudo cp /mnt/nutch-site.xml /usr/local/hadoop-$HADOOP_VERSION/conf/
sudo cp /mnt/nutch-default.xml /usr/local/hadoop-$HADOOP_VERSION/conf/
sudo cp -r /mnt/jars/* /usr/local/hadoop-$HADOOP_VERSION/lib/
#sudo rm -rf /usr/local/hadoop-$HADOOP_VERSION/lib/apache-nutch-1.7.jar
#downloading hbase
echo "downloading hbase......."
sudo wget -nv http://apache.claz.org/hbase/stable/hbase-0.94.11.tar.gz
sudo tar -zxvf hbase-$HBASE_VERSION.tar.gz
sudo rm -f hbase-$HBASE_VERSION.tar.gz

# Configure Hadoop
echo "setting java_home & hadoop_logdir"
sudo sed -i -e "s|# export JAVA_HOME=.*|export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}|" \
       -e "s|# export HADOOP_LOG_DIR=.*|export HADOOP_LOG_DIR=/mnt/hadoop/logs|" \
       -e "s|# export HADOOP_SLAVE_SLEEP=.*|export HADOOP_SLAVE_SLEEP=1|" \
	 -e "s|# export HADOOP_CLASSPATH=.*|export HADOOP_CLASSPATH=/usr/local/hadoop-${HADOOP_VERSION}/lib/*:/usr/local/hbase-$HBASE_VERSION/lib/*|" \
       -e "s|# export HADOOP_OPTS=.*|export HADOOP_OPTS=-server|" \
       /usr/local/hadoop-$HADOOP_VERSION/conf/hadoop-env.sh

# Configure Hbase      
echo "setting into hbase-env.sh"
sudo sed -i -e "s|# export JAVA_HOME=.*|export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}|" \
      -e "s|# export HBASE_CLASSPATH=.*|export HBASE_CLASSPATH=/usr/local/hbase-$HBASE_VERSION|" \
      -e "s|# export HBASE_MANAGES_ZK=.*|export HBASE_MANAGES_ZK=true|" \
       /usr/local/hbase-$HBASE_VERSION/conf/hbase-env.sh

#copy hbase jar into hadoop
echo "hbase jar into hadoop library"
sudo chmod -R 777 /usr/local/hadoop-$HADOOP_VERSION/lib
sudo cp /usr/local/hbase-$HBASE_VERSION/hbase-$HBASE_VERSION.jar /usr/local/hadoop-$HADOOP_VERSION/lib
#Run user data as script on instance startup
echo "changing mode of /etc/rc.local"
echo $SSH_OPTS
sudo chmod -R 777 /etc/rc.local
echo "to give permission for execution of /etc/rc.local"
sudo chmod +x /etc/init.d/ec2-run-user-data
echo "copying /etc/init.d/ec2-run-user-data into /etc/rc.local"
echo "/etc/init.d/ec2-run-user-data" >> /etc/rc.local
echo "change into previous mode of /etc/rc.local"
sudo chmod -R 751 /etc/rc.local

# Setup root user bash environment
echo " setting java_home etc into bashrc file"
echo "export JAVA_HOME=/usr/local/jdk${JAVA_VERSION}" >> /home/ubuntu/.bash_profile
echo "export HADOOP_HOME=/usr/local/hadoop-${HADOOP_VERSION}" >> /home/ubuntu/.bash_profile
echo "export PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$PATH" >> /home/ubuntu/.bash_profile
echo "export HBASE_HOME=/usr/local/hbase-${HBASE_VERSION}" >> /home/ubuntu/.bash_profile
# Configure networking.
# Delete SSH authorized_keys since it includes the key it was launched with. (Note that it is re-populated when an instance starts.)
#sudo rm -f /home/ubuntu/.ssh/authorized_keys

# Ensure logging in to new hosts is seamless.
echo "changing mode of ssh_config"
sudo chmod -R 777 /etc/ssh/ssh_config
echo "copying into ssh_config"
echo '    StrictHostKeyChecking no' >> /etc/ssh/ssh_config
sudo chmod -R 644 /etc/ssh/ssh_config
echo "success"

# Bundle and upload image
cd /home/ubuntu
# Don't need to delete .bash_history since it isn't written until exit.
df -h
echo "ec2-bundle-vol command"
sudo ec2-bundle-vol -d /mnt -k /mnt/pk*.pem -c /mnt/cert*.pem -u $AWS_ACCOUNT_ID -s 3072 -p hadoop-$HADOOP_VERSION-$ARCH -r $ARCH
echo "ec2-upload-bundle command"
sudo ec2-upload-bundle -b $S3_BUCKET -m /mnt/hadoop-$HADOOP_VERSION-$ARCH.manifest.xml -a $AWS_ACCESS_KEY_ID -s $AWS_SECRET_ACCESS_KEY

# End
echo Done

