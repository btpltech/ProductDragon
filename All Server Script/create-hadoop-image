#!/usr/bin/env bash

# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Create a Hadoop AMI.
# Inspired by Jonathan Siegel's EC2 script (http://blogsiegel.blogspot.com/2006/08/sandboxing-amazon-ec2.html)

# Import variables
bin=`dirname "$0"`
bin=`cd "$bin"; pwd`
. "$bin"/hadoop-ec2-env.sh

AMI_IMAGE=`ec2-describe-images -a | grep $S3_BUCKET | grep $HADOOP_VERSION | grep $ARCH | grep available | awk '{print $2}'`

[ ! -z $AMI_IMAGE ] && echo "AMI already registered, use: ec2-deregister $AMI_IMAGE" && exit -1

echo "Starting a AMI with ID $BASE_AMI_IMAGE."
OUTPUT=`ec2-run-instances $BASE_AMI_IMAGE -k $KEY_NAME -t $INSTANCE_TYPE`
BOOTING_INSTANCE=`echo $OUTPUT | awk '{print $6}'`

echo "Instance is $BOOTING_INSTANCE."

echo "Polling server status (ec2-describe-instances $BOOTING_INSTANCE)"
while true; do
  printf "."
  HOSTNAME=`ec2-describe-instances $BOOTING_INSTANCE | grep running | awk '{print $4}'`
  if [ ! -z $HOSTNAME ]; then
    break;
  fi
  sleep 1
done

echo "The server is available at $HOSTNAME."
while true; do
  REPLY=`ssh $SSH_OPTS "ubuntu@$HOSTNAME" 'echo "hello"'`
  if [ ! -z $REPLY ]; then
   break;
  fi
  sleep 5
done

#read -p "Login first? [yes or no]: " answer

if [ "$answer" == "yes" ]; then
echo "ssh login into hostname"
  ssh $SSH_OPTS "ubuntu@$HOSTNAME"
fi

echo "Copying scripts."

# Copy setup scripts
ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo chmod -R 777 /mnt
echo "permission given to mnt"

scp $SSH_OPTS "$bin"/hadoop-ec2-env.sh "ubuntu@$HOSTNAME:/mnt"
scp $SSH_OPTS "$bin"/image/create-hadoop-image-remote "ubuntu@$HOSTNAME:/mnt"

ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo chmod -R 777 /etc/init.d
echo "etc success"

scp $SSH_OPTS "$bin"/image/ec2-run-user-data "ubuntu@$HOSTNAME:/etc/init.d"
ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo chmod -R 751 /etc/init.d

echo "etc closed"
# Copy private key and certificate (for bundling image)
echo "copying pk and cert into /mnt"
scp $SSH_OPTS $EC2_KEYDIR/pk*.pem "ubuntu@$HOSTNAME:/mnt"
scp $SSH_OPTS $EC2_KEYDIR/cert*.pem "ubuntu@$HOSTNAME:/mnt"
scp $SSH_OPTS $AMPLE_HOME/nutch/nutch-default.xml "ubuntu@$HOSTNAME:/mnt"
scp $SSH_OPTS $AMPLE_HOME/nutch/nutch-site.xml "ubuntu@$HOSTNAME:/mnt"
ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo mkdir /mnt/jars
ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo chmod -R 777 /mnt/jars
scp $SSH_OPTS $AMPLE_HOME/lib/aws-java-sdk-1.3.10.jar "ubuntu@$HOSTNAME:/mnt/jars"
#scp $SSH_OPTS $AMPLE_HOME/lib/apache-nutch-1.7.jar "ubuntu@$HOSTNAME:/mnt/jars"
scp $SSH_OPTS $AMPLE_HOME/lib/gson-2.2.3.jar "ubuntu@$HOSTNAME:/mnt/jars"
scp $SSH_OPTS $AMPLE_HOME/lib/htmlcleaner-2.2.1.jar "ubuntu@$HOSTNAME:/mnt/jars"
#ssh $SSH_OPTS "ubuntu@$HOSTNAME" sudo cp -r /mnt/jars/* "/usr/local/hadoop-$HADOOP_VERSION/lib/"
# Connect to it
echo "ssh login to /mnt/create-hadoop-image-remote file"
ssh $SSH_OPTS "ubuntu@$HOSTNAME" '/mnt/create-hadoop-image-remote'

# Register image
echo "registering image into S3 bucket"
ec2-register $S3_BUCKET/hadoop-$HADOOP_VERSION-$ARCH.manifest.xml

echo "Terminate with: ec2-terminate-instances $BOOTING_INSTANCE"
